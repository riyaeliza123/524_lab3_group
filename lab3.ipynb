{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 541 Lab 3 - Bias and fairness\n",
    "\n",
    "## Required readings\n",
    "\n",
    "We covered some of these videos in lab, but including them here so that you have everything in one place. There might be questions on specifics from the required readings whereas the optional readings are more of a general help, optional questions, and if you are interested to explore further.\n",
    "\n",
    "- Princeton AI Ethics Case study [\"Hiring by machine\"](https://aiethics.princeton.edu/wp-content/uploads/sites/587/2018/12/Princeton-AI-Ethics-Case-Study-5.pdf) (35 min read)\n",
    "- **Lec 5** Latanya Sweeney [\"How Technology Will Dictate Our Civic Future\"](https://www.youtube.com/watch?v=GOyiRcCwEi8&t?t=158) (02:38 - 08:15 and 11:52 - 16:24)\n",
    "- **Lec 5** Michele Gilman, Mary Madden, & Alicia Luchett [\"Privacy and Poverty\"](https://www.youtube.com/watch?v=Upzzevv0Mag&t=408s) (06:48 - 12:56)\n",
    "- **Lec 6** Ava Soleimany [\"AI Bias and Fairness\"](https://www.youtube.com/watch?v=wmyVODy_WD8) (04:23 - 08:32)\n",
    "- **Lec 6 (partly)** Joy Buolamwini [\"The Coded Gaze: Bias in Artificial Intelligence\"](https://www.youtube.com/watch?v=eRUEVYndh9c) (13 min video)\n",
    "\n",
    "## Optional readings\n",
    "\n",
    "<details><summary>Click to show</summary>\n",
    "\n",
    "You don't have to read these before lab and you don't have to read all of them. The main reason for including them here is as a recommendation of where to deepen your knowledge if there is something that you find particularly interesting. I am giving a brief background to each so you can pick the ones that peak your interest.\n",
    "\n",
    "- Rachel Thomas [\"Bias and fairness\"](https://www.youtube.com/watch?v=mG-cTS3fnnw)\n",
    "- Michael Wick [\"Unlocking Fairness: A tradeoff revisited\"](https://blogs.oracle.com/ai-and-datascience/post/unlocking-fairness-a-trade-off-revisited)\n",
    "\n",
    "</details>\n",
    "<!-- - [\"On fairness and calibration\"](https://arxiv.org/pdf/1709.02012.pdf (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6594166/) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "rubric={mechanics:20}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<p>You receive marks for submitting your lab correctly, please follow these instructions:</p>\n",
    "\n",
    "<ul>\n",
    "  <li><a href=\"https://ubc-mds.github.io/resources_pages/general_lab_instructions/\">\n",
    "      Follow the general lab instructions.</a></li>\n",
    "  <li><a href=\"https://github.com/UBC-MDS/public/tree/master/rubric\">\n",
    "      Click here to view a description of the rubrics used to grade the questions</a></li>\n",
    "  <li>Push your <code>.ipynb</code> file to your GitHub repository for this lab (make at least three commits).</li>\n",
    "  <li>Upload your <code>.ipynb</code> file to Gradescope.\n",
    "  </li>\n",
    "  <li>Include a clickable link to your GitHub repo for the lab just below this cell\n",
    "    <ul>\n",
    "      <li>It should look something like this https://github.ubc.ca/MDS-2022-23/DSCI_541_labX_yourcwl.</li>\n",
    "      <li>If you are working in a group, you can create you own (public) repo in <a href=\"https://github.ubc.ca/MDS-2023-24\"> the UBC-MDS organization</a> and link that instead.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "<li>All your written answers must be in your own words.</li>\n",
    "<li>You are not allowed to use generative AI tools to write your answers for you or simply paraphrase answer that you generate from these tools (that will lead to a failing grade), but you can use them to further understand the topics you are learning about.</li>\n",
    " \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/riyaeliza123/541_lab3_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall writing quality\n",
    "rubric={writing:20}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<p>You will receive an overall writing grade for the entire lab instead of for each question. This is just a small part of your total grade, but please use the Jupyter Lab spell checker extension to catch typos and read through your text for grammatical errors before submitting (or paste it into Google Docs/MS Word/Grammarly. You don't need to type anything under this cell, it is just a placeholder to generate the grading rubric.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Short answer questions\n",
    "\n",
    "Keep your replies brief, 1-3 sentences per question. Although these are short answer questions, don't copy answers from the readings, use your own words so that you practice learning these concepts. These will not be discussed during the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1\n",
    "rubric={reasoning:60}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<ol type=\"1\">\n",
    "<li>What is historical bias?</li>\n",
    "<li>What is representation bias?</li>\n",
    "<li>What is measurement bias?</li>\n",
    "<li>What is aggregation bias?</li>\n",
    "<li>What is evaluation bias?</li>\n",
    "<li>What is deployment bias?</li>\n",
    "</ol>\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Historical bias refers to the bias created in data cause due to reflection of current state of the world. it refers to the bias that is present in training data due to past and existing societal biases. It may lead to a model that produces harmful outcomes for certain groups of people.\n",
    "2. Representation bias occurs when the data used to train a model is not a true reflection of the broader population or the ground truth. In other words, the dataset used for training does not adequately represent the diversity or variability present in the real-world scenario that the model is meant to generalize to.\n",
    "3. Measurement bias occurs when the metrics used to evaluate or measure a certain phenomenon are inaccurate, or not truly representative of the underlying concept. This type of bias can lead to misleading conclusions and affect the validity and reliability of measurements.\n",
    "4. Aggregation bias occurs when a single, generalized model is applied to data that contains sub-populations with distinct characteristics or behaviors. In other words, it occurs when variations within different subgroups are overlooked, and a one-size-fits-all approach is used, leading to potential inaccuracies in predictions or analyses.\n",
    "5. Evaluation bias occurs when the test or benchmark data used to assess the performance of a model does not adequately represent the broader population or use case for which the model is intended. This type of bias can lead to the development and deployment of models that perform well only on a specific subset of the data represented by the benchmark, but may not generalize well to other scenarios. These aggregate measures can sometimes miss the underlying biases and can hide subgroup underperformances.\n",
    "6. Deployment bias occurs when there is a mismatch between a problem that a model is intended to solve versus how it is actually used. This discrepancy is often attributed to the complex sociotechnical systems and institutional structures in which the model operates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Discussion questions\n",
    "\n",
    "This section asks you to expand a bit on your reasoning, but still aim to write succinct replies around one paragraph per sub-question. The goal of lab discussions are not to provide you with the right answers, but to help your discussion along. Your TA will assist in this by bringing up topics that you might not have thought of, ask questions to break the silence or a dead end, and move the conversation along so that you have time to go through most questions. How useful the lab discussion is for your submission ultimately relies on that you actively contribute to the discussion and help your peers contribute and exchange ideas.\n",
    "\n",
    "##  Some tips to make your discussions in lab more effective\n",
    "\n",
    "It is easy to overlook the flaws of our own reasoning,\n",
    "so having a discussion with colleagues is an excellent opportunity\n",
    "to develop your thinking and receive feedback\n",
    "from someone who can provide an alternative perspective from your own.\n",
    "Nevertheless,\n",
    "many people don't know how to have an effective discussion,\n",
    "so I am sharing a few tips for you to be able to make the most out of this opportunity:\n",
    "\n",
    "- Commit to learning, not \"winning\" debates. \n",
    "- Comment in order to share information and develop arguments further, not to persuade.\n",
    "- Listen respectfully, without interrupting, to try to understand each others' views.\n",
    "    - Don't focus on what you are going to say next while someone else is talking.\n",
    "- Challenge ideas, not individuals.\n",
    "    - And be open to having your own ideas challenged.\n",
    "- Think about as good arguments as possible against your position.\n",
    "    - This is especially useful if many of your peers have the same opinion, help your group find angles that you might otherwise be missing.\n",
    "- Allow everyone the chance to speak.\n",
    "    - Politely ask members of your group about their opinion.\n",
    "- Avoid assumptions about any member of the class or generalizations about social groups.\n",
    "    - Be careful about asking individuals to speak on the behalf of their (perceived) social group.\n",
    "- Be aware of [logical fallacies](https://blog.hubspot.com/marketing/common-logical-fallacies), but avoid pointing them out in rude or disrespectful ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 -- Hiring by machine\n",
    "rubric={reasoning:100}\n",
    " \n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<ol type=\"1\">\n",
    "<li>PARiS’ lists of suggested applicants closely resembled the lists that would have been drafted by Strategeion’s human HR team. To the extent that PARiS was biased towards a particular kind of applicant, this suggests that the human HR workers were as well. Are there any additional considerations/dangers of algorithmic biases compared to human ones in a scenario like this? Could having a \"human in the loop\" mitigate these biases and how could that be implemented effectively given the large number of applicants?</li>\n",
    "<li>Biased data pose a problem for ensuring fairness in AI systems. Given the company’s demographics, what could Strategeion’s engineers have done to counteract the skewed employee data? To what extent do you think such proactive efforts are the responsibility of individual engineers or engineering teams?</li>\n",
    "<li>Job interview companies <a href=https://www.pymetrics.ai/>Pymetrics</a> and <a href=https://www.hirevue.com/>Hirevue</a> implement games from psychology research to more accurately determine the qualities of the applicants. They can also use facial recognition software to detect and judge emotional reactions during interviews. Both companies have statements around their focus on fairness in the interview process and work to eliminate human bias. What do you see as the main potential advantages and possible dangers with hiring interviews conducted this way and do you think such companies will improve the hiring process overall?</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Algorithmic bias have many concerns than human bias because of many reasons. Algorithmic bias are scalable and can reinforce themselves without intervention of humans.Algorithmic decisions are typically applied at a larger scale than human decisions, amplifying the impact of any biases present. Because of its self reinforcement learning, it learns from the data and take its own decisions which is more dangerous than human bias. In the context of hiring, humans will have a different persceptive while seeing the resumes. If we ask the biased ML model to look into 1000 resumes, then the whole subset of resume will be affected. Its because, once the model got trained and ready, it learns itself.\n",
    "Incorporating a human in the loop can certainly help mitigate these biases. Humans can provide qualitative insights, moral reasoning, and empathy in decision-making processes that the alogirthm cannot make by itself. Human intervention, can help to reduce the bias and it can also help to avoid significant consequences (like in PARiS, without much human intervention they have faced consequences like legal action case). In the context of hiring, the recruiter must understand that algorithm will have certain bias and take that into consideration in training the model and feeding the data to the model. Human interventions can help to overcome these issues in building a fair model.\n",
    "\n",
    "2. In Strategion's scenario, to counteract the skewed employee data, the engineers have to start from handling training data to avoid the any type of biases. One way they can do is to, train the model with various datasets which includes resumes from different backgrounds instead of one specific background. By doing this way, we can compensate for representing data with single group. Second thing what they can do is, they can set the confidence intervals while training the model. So that the model can give score for each applicant, based on the scores the recruiters can check the resumes manually and select the candidates.\n",
    "Its not only the responsibility of engineers for fairness in AI, its the responsibility of individual engineers, engineering teams and whole organization. Individual engineers has the responsibility to recognize any potential bias in the data or not and have to ensure that data is not biased. The highest level of responsibility lies with organization  where they have to set some measures for an ethical AI development culture, provide training and resources for fairness and bias reduction, and set strategic priorities that value fairness in AI.\n",
    "\n",
    "3. Using pymetrics and Hirevue for hiring has its own advantages and disadvantages.\n",
    "Advantages:\n",
    "It can help to reduce human subjective bias. These can help to identify the candidates emotions without showing any bias to any candidate, as AI model dont have different emotions which is helful to make unique decisions. These can work as a good supervising model which can tackle a large number of candidates thereby reducing the cost of extra resources. These tools are beneficial for the companies who receive a lot of applications.\n",
    "Disadvantages:\n",
    "These tools have its own cons. One of the disadvantage is that, if the model has trained with skewed data then it can be biased. The underlying technology used in these tools is facial recognition, which has its downsides also. We are not aware of how this data is being used and where it is used which leads to privacy concerns.\n",
    "\n",
    "I think that these tools have the potential to improve the hiring process by making it more efficient, and potentially less biased. Oraganizations must transparently communicate how candidate data is collected, used, and protected which includes complying with data protection regulations. However these tools can be used to assist humans in decision making rather than replace thereby ensuring that the final hiring decisions are enriched by both AI and humans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 -- Bias trade-offs and considerations\n",
    "rubric={reasoning:100}\n",
    " \n",
    "<div class=\"alert alert-info\">\n",
    " \n",
    "<ol type=\"1\">\n",
    "<li>We covered some ways of how algorithmic bias has the potential to inflict more harm than human bias (scalability, automation, self-reinforcement, etc). Can you identify some properties of algorithmic bias that are favorable compared to human bias for addressing the root cause of the problem? In other words, could a biased algorithm be more useful than a biased human for understanding where the bias is coming from and how to reduce it?</li>\n",
    "<li>A company has created a software solution to predict the prevalence of a rare genetic disease based on automated analysis of skeletal structure from X-ray images. They claim that their algorithm can detect a skeletal structural defect which is a common symptom of the rare disease and they report a 99.9% accuracy on their training and evaluation data. You are tasked with auditing this algorithm to determine whether it has any potentially harmful biases or if it can safely be employed in the national health care system. Go through the six biases from question 1.1 and explain which ones you would take into account when evaluating the company's product. For each bias either explain why you want not include it or give a specific example for how this bias could apply to the scenario here and what questions you might want to explore/ask the company during your assessment?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.\n",
    "\n",
    "**a) Historical Bias:** Historical bias in our analysis stems from the current and past state of the world. While medical data, specifically X-ray images, may exhibit representation biases, it differs from text data in that it lacks biases linked to past and present societal issues. Additionally, changes and advancements in X-ray methods over time may introduce historical bias related to image quality, yet this does not introduce societal biases. Hence, we won’t have any prominent historical bias.\n",
    "\n",
    "**b) Representation Bias:** There are many ways in which representational bias might kick-in.\n",
    "i) Under-represented groups: We would want to train a generalized model for detecting the disease but the image data would primarily belong to certain races and ethnicities who have had better access to healthcare.\n",
    "ii) The training data does not reflect the end user population. It is possible that while training a universal model for disease detection, the skeletal structure defect may not manifest in X-rays of individuals with Asian or African genetic variations. Consequently, applying this generalized model to these populations may not produce the anticipated or accurate results.\n",
    "\n",
    "Questions to ask or explore:\n",
    "- How was the dataset collected, and does it capture the variability present in the real-world scenario?\n",
    "- Are there any demographic or regional biases in the dataset that could affect the model's performance for specific populations?\n",
    "\n",
    "**c) Measurement Bias:** Measurement bias can arise at various stages of the algorithm development process:\n",
    "- Preprocessing: The preprocessing of image data involves steps like normalization, resizing, and data augmentation. There is a risk of measurement biases entering the process if the incorrect methods are selected.\n",
    "- Model Training: Selecting the appropriate architecture for training the model is crucial during the model training phase. Commonly used CNN models have various variations tailored to specific modeling tasks, and choosing the right model is essential to avoid measurement bias.\n",
    "- Metrics: The selection of metrics is critical, depending on whether the dataset is balanced or not. For instance, in the case of an imbalanced dataset, achieving an accuracy of 99.9% may not accurately represent the model's performance for minority classes. The precision and recall scores may be disproportionately low due to the dataset imbalance, leading to representation bias.\n",
    "\n",
    "Questions to ask or explore:\n",
    "- What are the measurement techniques used to identify skeletal structural defects, and how reliable are they?\n",
    "\n",
    "**d) Aggregation Bias:** We should examine whether a universally applicable model is being employed for our data and determine if there are any inherent subgroups that warrant separate consideration. For instance, genetics significantly influences immunity to diseases, and within our data, certain ethnicities or genetic variations may exhibit heightened resistance to this rare-genetic disease. For instance, individuals from Black African, African Caribbean, and South Asian (Indian, Pakistani, Bangladeshi) backgrounds may face an elevated risk of developing type 2 diabetes at a younger age.\n",
    "\n",
    "Questions to ask or explore:\n",
    "- Does the model account for variations in skeletal structure within different demographic or genetic subpopulations?\n",
    "- How does the algorithm handle cases where there are distinct characteristics that might affect the prediction differently?\n",
    "\n",
    "**e) Evaluation Bias:** Evaluate whether the test or benchmark data used to assess the model's performance adequately represents the broader population or use case. If the evaluation data is not diverse, the model may appear more accurate than it truly is.\n",
    "\n",
    "Questions to ask or explore:\n",
    "- How was the evaluation dataset selected, and does it cover a diverse range of cases and scenarios?\n",
    "- What evaluation metrics were used? Are these metrics hiding any disparities in other types of errors (eg. false positive rate)?\n",
    "\n",
    "**f) Deployment Bias:** There is no deployment bias yet as the model is trained to detect the rare genetic disease using the skeletal images and is being used, audited and tested for that exact same purpose in the national health care system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3 (Challenging)\n",
    "rubric={reasoning:15}\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "<ol type=\"1\">\n",
    "    \n",
    "<li>Representation bias and evaluation bias can both lead to poor performance on specific subgroups due to under-representation in the sampled data. In many cases it is possible to raise the model performance both for these subgroups and as a whole by balancing the data set. However, sometimes making the performance more equal between subgroups could come at the expense of overall model performance. What are good guidelines for when this trade-off is worthwhile?</li>\n",
    "<li>Read an article of your choice (not from the lecture slides) relating to one of the biases that we discussed during lecture (or another important machine learning bias if you think something was left out). Provide a link to the article and explain the type of bias that occurred in the situation the article is describing as well as provide solutions for how you think that bias could have been mitigated.</li>\n",
    "</ol>\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE\n",
    "\n",
    "1.\n",
    "\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "feedback"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "# Help us improve the labs\n",
    "\n",
    "The MDS program is continually looking to improve our courses, including lab questions and content. The following optional questions will not affect your grade in any way nor will they be used for anything other than program improvement:\n",
    "\n",
    "1. Approximately how many hours did you spend working or thinking about this assignment (including lab time)?\n",
    "\n",
    "#Ans:\n",
    "\n",
    "2. Were there any questions that you particularly liked or disliked?\n",
    "\n",
    "#Ans: [Questions you liked]\n",
    "\n",
    "#Ans: [Questions you disliked]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
