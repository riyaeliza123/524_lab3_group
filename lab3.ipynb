{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 541 Lab 3 - Bias and fairness\n",
    "\n",
    "## Required readings\n",
    "\n",
    "We covered some of these videos in lab, but including them here so that you have everything in one place. There might be questions on specifics from the required readings whereas the optional readings are more of a general help, optional questions, and if you are interested to explore further.\n",
    "\n",
    "- Princeton AI Ethics Case study [\"Hiring by machine\"](https://aiethics.princeton.edu/wp-content/uploads/sites/587/2018/12/Princeton-AI-Ethics-Case-Study-5.pdf) (35 min read)\n",
    "- **Lec 5** Latanya Sweeney [\"How Technology Will Dictate Our Civic Future\"](https://www.youtube.com/watch?v=GOyiRcCwEi8&t?t=158) (02:38 - 08:15 and 11:52 - 16:24)\n",
    "- **Lec 5** Michele Gilman, Mary Madden, & Alicia Luchett [\"Privacy and Poverty\"](https://www.youtube.com/watch?v=Upzzevv0Mag&t=408s) (06:48 - 12:56)\n",
    "- **Lec 6** Ava Soleimany [\"AI Bias and Fairness\"](https://www.youtube.com/watch?v=wmyVODy_WD8) (04:23 - 08:32)\n",
    "- **Lec 6 (partly)** Joy Buolamwini [\"The Coded Gaze: Bias in Artificial Intelligence\"](https://www.youtube.com/watch?v=eRUEVYndh9c) (13 min video)\n",
    "\n",
    "## Optional readings\n",
    "\n",
    "<details><summary>Click to show</summary>\n",
    "\n",
    "You don't have to read these before lab and you don't have to read all of them. The main reason for including them here is as a recommendation of where to deepen your knowledge if there is something that you find particularly interesting. I am giving a brief background to each so you can pick the ones that peak your interest.\n",
    "\n",
    "- Rachel Thomas [\"Bias and fairness\"](https://www.youtube.com/watch?v=mG-cTS3fnnw)\n",
    "- Michael Wick [\"Unlocking Fairness: A tradeoff revisited\"](https://blogs.oracle.com/ai-and-datascience/post/unlocking-fairness-a-trade-off-revisited)\n",
    "\n",
    "</details>\n",
    "<!-- - [\"On fairness and calibration\"](https://arxiv.org/pdf/1709.02012.pdf (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6594166/) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "rubric={mechanics:20}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<p>You receive marks for submitting your lab correctly, please follow these instructions:</p>\n",
    "\n",
    "<ul>\n",
    "  <li><a href=\"https://ubc-mds.github.io/resources_pages/general_lab_instructions/\">\n",
    "      Follow the general lab instructions.</a></li>\n",
    "  <li><a href=\"https://github.com/UBC-MDS/public/tree/master/rubric\">\n",
    "      Click here to view a description of the rubrics used to grade the questions</a></li>\n",
    "  <li>Push your <code>.ipynb</code> file to your GitHub repository for this lab (make at least three commits).</li>\n",
    "  <li>Upload your <code>.ipynb</code> file to Gradescope.\n",
    "  </li>\n",
    "  <li>Include a clickable link to your GitHub repo for the lab just below this cell\n",
    "    <ul>\n",
    "      <li>It should look something like this https://github.ubc.ca/MDS-2022-23/DSCI_541_labX_yourcwl.</li>\n",
    "      <li>If you are working in a group, you can create you own (public) repo in <a href=\"https://github.ubc.ca/MDS-2023-24\"> the UBC-MDS organization</a> and link that instead.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "<li>All your written answers must be in your own words.</li>\n",
    "<li>You are not allowed to use generative AI tools to write your answers for you or simply paraphrase answer that you generate from these tools (that will lead to a failing grade), but you can use them to further understand the topics you are learning about.</li>\n",
    " \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/riyaeliza123/541_lab3_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall writing quality\n",
    "rubric={writing:20}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<p>You will receive an overall writing grade for the entire lab instead of for each question. This is just a small part of your total grade, but please use the Jupyter Lab spell checker extension to catch typos and read through your text for grammatical errors before submitting (or paste it into Google Docs/MS Word/Grammarly. You don't need to type anything under this cell, it is just a placeholder to generate the grading rubric.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Short answer questions\n",
    "\n",
    "Keep your replies brief, 1-3 sentences per question. Although these are short answer questions, don't copy answers from the readings, use your own words so that you practice learning these concepts. These will not be discussed during the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1\n",
    "rubric={reasoning:60}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<ol type=\"1\">\n",
    "<li>What is historical bias?</li>\n",
    "<li>What is representation bias?</li>\n",
    "<li>What is measurement bias?</li>\n",
    "<li>What is aggregation bias?</li>\n",
    "<li>What is evaluation bias?</li>\n",
    "<li>What is deployment bias?</li>\n",
    "</ol>\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Historical bias refers to the bias created in data cause due to reflection of current state of the world. it refers to the bias that is present in training data due to past and existing societal biases. It may lead to a model that produces harmful outcomes for certain groups of people.\n",
    "2. Representation bias occurs when the data used to train a model is not a true reflection of the broader population or the ground truth. In other words, the dataset used for training does not adequately represent the diversity or variability present in the real-world scenario that the model is meant to generalize to.\n",
    "3. Measurement bias occurs when the metrics used to evaluate or measure a certain phenomenon are inaccurate, or not truly representative of the underlying concept. This type of bias can lead to misleading conclusions and affect the validity and reliability of measurements.\n",
    "4. Aggregation bias occurs when a single, generalized model is applied to data that contains sub-populations with distinct characteristics or behaviors. In other words, it occurs when variations within different subgroups are overlooked, and a one-size-fits-all approach is used, leading to potential inaccuracies in predictions or analyses.\n",
    "5. Evaluation bias occurs when the test or benchmark data used to assess the performance of a model does not adequately represent the broader population or use case for which the model is intended. This type of bias can lead to the development and deployment of models that perform well only on a specific subset of the data represented by the benchmark, but may not generalize well to other scenarios. These aggregate measures can sometimes miss the underlying biases and can hide subgroup underperformances.\n",
    "6. Deployment bias occurs when there is a mismatch between a problem that a model is intended to solve versus how it is actually used. This discrepancy is often attributed to the complex sociotechnical systems and institutional structures in which the model operates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Discussion questions\n",
    "\n",
    "This section asks you to expand a bit on your reasoning, but still aim to write succinct replies around one paragraph per sub-question. The goal of lab discussions are not to provide you with the right answers, but to help your discussion along. Your TA will assist in this by bringing up topics that you might not have thought of, ask questions to break the silence or a dead end, and move the conversation along so that you have time to go through most questions. How useful the lab discussion is for your submission ultimately relies on that you actively contribute to the discussion and help your peers contribute and exchange ideas.\n",
    "\n",
    "##  Some tips to make your discussions in lab more effective\n",
    "\n",
    "It is easy to overlook the flaws of our own reasoning,\n",
    "so having a discussion with colleagues is an excellent opportunity\n",
    "to develop your thinking and receive feedback\n",
    "from someone who can provide an alternative perspective from your own.\n",
    "Nevertheless,\n",
    "many people don't know how to have an effective discussion,\n",
    "so I am sharing a few tips for you to be able to make the most out of this opportunity:\n",
    "\n",
    "- Commit to learning, not \"winning\" debates. \n",
    "- Comment in order to share information and develop arguments further, not to persuade.\n",
    "- Listen respectfully, without interrupting, to try to understand each others' views.\n",
    "    - Don't focus on what you are going to say next while someone else is talking.\n",
    "- Challenge ideas, not individuals.\n",
    "    - And be open to having your own ideas challenged.\n",
    "- Think about as good arguments as possible against your position.\n",
    "    - This is especially useful if many of your peers have the same opinion, help your group find angles that you might otherwise be missing.\n",
    "- Allow everyone the chance to speak.\n",
    "    - Politely ask members of your group about their opinion.\n",
    "- Avoid assumptions about any member of the class or generalizations about social groups.\n",
    "    - Be careful about asking individuals to speak on the behalf of their (perceived) social group.\n",
    "- Be aware of [logical fallacies](https://blog.hubspot.com/marketing/common-logical-fallacies), but avoid pointing them out in rude or disrespectful ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 -- Hiring by machine\n",
    "rubric={reasoning:100}\n",
    " \n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<ol type=\"1\">\n",
    "<li>PARiS’ lists of suggested applicants closely resembled the lists that would have been drafted by Strategeion’s human HR team. To the extent that PARiS was biased towards a particular kind of applicant, this suggests that the human HR workers were as well. Are there any additional considerations/dangers of algorithmic biases compared to human ones in a scenario like this? Could having a \"human in the loop\" mitigate these biases and how could that be implemented effectively given the large number of applicants?</li>\n",
    "<li>Biased data pose a problem for ensuring fairness in AI systems. Given the company’s demographics, what could Strategeion’s engineers have done to counteract the skewed employee data? To what extent do you think such proactive efforts are the responsibility of individual engineers or engineering teams?</li>\n",
    "<li>Job interview companies <a href=https://www.pymetrics.ai/>Pymetrics</a> and <a href=https://www.hirevue.com/>Hirevue</a> implement games from psychology research to more accurately determine the qualities of the applicants. They can also use facial recognition software to detect and judge emotional reactions during interviews. Both companies have statements around their focus on fairness in the interview process and work to eliminate human bias. What do you see as the main potential advantages and possible dangers with hiring interviews conducted this way and do you think such companies will improve the hiring process overall?</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 -- Bias trade-offs and considerations\n",
    "rubric={reasoning:100}\n",
    " \n",
    "<div class=\"alert alert-info\">\n",
    " \n",
    "<ol type=\"1\">\n",
    "<li>We covered some ways of how algorithmic bias has the potential to inflict more harm than human bias (scalability, automation, self-reinforcement, etc). Can you identify some properties of algorithmic bias that are favorable compared to human bias for addressing the root cause of the problem? In other words, could a biased algorithm be more useful than a biased human for understanding where the bias is coming from and how to reduce it?</li>\n",
    "<li>A company has created a software solution to predict the prevalence of a rare genetic disease based on automated analysis of skeletal structure from X-ray images. They claim that their algorithm can detect a skeletal structural defect which is a common symptom of the rare disease and they report a 99.9% accuracy on their training and evaluation data. You are tasked with auditing this algorithm to determine whether it has any potentially harmful biases or if it can safely be employed in the national health care system. Go through the six biases from question 1.1 and explain which ones you would take into account when evaluating the company's product. For each bias either explain why you want not include it or give a specific example for how this bias could apply to the scenario here and what questions you might want to explore/ask the company during your assessment?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE\n",
    "\n",
    "1.\n",
    "\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3 (Challenging)\n",
    "rubric={reasoning:15}\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "<ol type=\"1\">\n",
    "    \n",
    "<li>Representation bias and evaluation bias can both lead to poor performance on specific subgroups due to under-representation in the sampled data. In many cases it is possible to raise the model performance both for these subgroups and as a whole by balancing the data set. However, sometimes making the performance more equal between subgroups could come at the expense of overall model performance. What are good guidelines for when this trade-off is worthwhile?</li>\n",
    "<li>Read an article of your choice (not from the lecture slides) relating to one of the biases that we discussed during lecture (or another important machine learning bias if you think something was left out). Provide a link to the article and explain the type of bias that occurred in the situation the article is describing as well as provide solutions for how you think that bias could have been mitigated.</li>\n",
    "</ol>\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWERS HERE\n",
    "\n",
    "1.\n",
    "\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "feedback"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "# Help us improve the labs\n",
    "\n",
    "The MDS program is continually looking to improve our courses, including lab questions and content. The following optional questions will not affect your grade in any way nor will they be used for anything other than program improvement:\n",
    "\n",
    "1. Approximately how many hours did you spend working or thinking about this assignment (including lab time)?\n",
    "\n",
    "#Ans:\n",
    "\n",
    "2. Were there any questions that you particularly liked or disliked?\n",
    "\n",
    "#Ans: [Questions you liked]\n",
    "\n",
    "#Ans: [Questions you disliked]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:531]",
   "language": "python",
   "name": "conda-env-531-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
